[package]
name = "anno"
version = "0.2.0"
edition = "2021"
rust-version = "1.75"
license = "MIT OR Apache-2.0"
description = "Information extraction for Rust: NER, coreference resolution, and evaluation. Supports regex patterns, transformer models (BERT, GLiNER, GLiNER2), and coreference resolution with comprehensive metrics."
keywords = ["ner", "nlp", "entity", "extraction", "coreference"]
categories = ["text-processing", "science"]
repository = "https://github.com/arclabs561/anno"
homepage = "https://docs.rs/anno"
documentation = "https://docs.rs/anno"
readme = "README.md"
authors = ["Arc <attobop@gmail.com>"]

[dependencies]
serde = { version = "1", features = ["derive"] }
serde_json = "1"
thiserror = "2"
regex = "1"
chrono = { version = "0.4", features = ["serde"] }
log = "0.4"
once_cell = "1"

# CLI dependencies (optional, for the anno binary)
clap = { version = "4", features = ["derive", "env", "wrap_help"], optional = true }
clap_complete = { version = "4", optional = true }
is-terminal = { version = "0.4", optional = true }

# Robustness module needs rand (optional, only with eval-advanced)
rand = { version = "0.9", optional = true }

# Dataset downloading (optional, eval-advanced)
ureq = { version = "2", optional = true }
sha2 = { version = "0.10", optional = true }

# Cache directory discovery (optional, eval)
dirs = { version = "5", optional = true }

# ML backends (optional)
ort = { version = "2.0.0-rc.10", optional = true }
tokenizers = { version = "0.22", optional = true }
hf-hub = { version = "0.4", optional = true }
ndarray = { version = "0.16", optional = true }
lru = { version = "0.12", optional = true }
candle-core = { version = "0.9", optional = true }
candle-nn = { version = "0.9", optional = true }
candle-transformers = { version = "0.9", optional = true }
# PyTorch model conversion (for candle backend) - now uses Python script via uv
safetensors = { version = "0.4", optional = true }  # For saving converted models
# gliner = { version = "0.1", optional = true } # Not yet published to crates.io

# Async inference support (optional)
tokio = { version = "1", features = ["rt", "sync"], optional = true }
futures = { version = "0.3", optional = true }

# High-throughput session pooling (optional)
crossbeam-channel = { version = "0.5", optional = true }

# Better mutex performance (optional, replaces std::sync::Mutex)
parking_lot = { version = "0.12", optional = true }

# Parallel processing for evaluation (optional)
rayon = { version = "1", optional = true }

[dev-dependencies]
criterion = "0.5"
proptest = "1"
tempfile = "3"
rand = "0.9"

[features]
# Minimal by default - users opt-in to what they need
default = []

# === Evaluation Framework (tiered) ===
# Core evaluation: P/R/F1, GoldEntity, basic metrics
# Includes dirs for platform cache directory discovery
eval = ["dep:dirs"]
# Bias analysis: gender, demographic, temporal, length bias evaluation
eval-bias = ["eval"]
# Advanced analysis: calibration, robustness, OOD, dataset download, etc.
# Includes ureq for downloading real NER datasets (WikiGold, WNUT, etc.)
eval-advanced = ["eval", "dep:rand", "dep:ureq", "dep:sha2"]
# Everything in eval
eval-full = ["eval", "eval-bias", "eval-advanced"]
# Parallel evaluation (faster processing on multi-core systems)
eval-parallel = ["eval", "dep:rayon"]
# Profiling instrumentation for performance analysis
eval-profiling = ["eval"]

# === Discourse Analysis ===
# Abstract anaphora resolution, event extraction, shell nouns
discourse = ["eval"]

# === ML Backends ===
onnx = ["dep:ort", "dep:tokenizers", "dep:hf-hub", "dep:ndarray", "dep:lru"]
candle = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:tokenizers", "dep:hf-hub", "dep:safetensors"]

# GPU acceleration for Candle (opt-in)
metal = ["candle", "candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]
cuda = ["candle", "candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
# gliner = ["dep:gliner", "dep:hf-hub"] # Not yet published

# === Production Features ===
# Async inference support (wrap blocking calls in spawn_blocking)
async-inference = ["dep:tokio", "dep:futures", "onnx"]
# High-throughput session pooling for concurrent inference
session-pool = ["dep:crossbeam-channel", "onnx"]
# Better mutex performance (parking_lot instead of std::sync::Mutex)
fast-lock = ["dep:parking_lot"]

# Everything
full = ["eval-full", "onnx", "candle", "discourse"]

# Production bundle (async + pooling + fast locks)
production = ["onnx", "async-inference", "session-pool", "fast-lock"]

# CLI binary feature
cli = ["dep:clap", "dep:clap_complete", "dep:is-terminal"]

# =============================================================================
# CLI BINARY
# =============================================================================
[[bin]]
name = "anno"
path = "src/bin/anno.rs"
required-features = ["cli"]

[[bin]]
name = "anno-eval"
path = "src/bin/anno_eval.rs"
required-features = ["eval"]

[[bench]]
name = "ner"
harness = false

[[bench]]
name = "evaluation_parallel"
harness = false
required-features = ["eval", "eval-parallel"]

[[bench]]
name = "gliner_profiling"
harness = false
required-features = ["onnx"]

[lints.rust]
missing_docs = "warn"

[lints.clippy]
unwrap_used = "warn"

# =============================================================================
# EXAMPLES with required features
# =============================================================================
# Examples - Run with: cargo run --example <name> --features <required-features>
# =============================================================================

[[example]]
name = "quickstart"
required-features = ["eval"]

[[example]]
name = "bert"
required-features = ["onnx"]

[[example]]
name = "candle"
required-features = ["candle"]

[[example]]
name = "gliner_candle"
required-features = ["candle"]

[[example]]
name = "download_models"
required-features = ["onnx"]  # Uses hf-hub for model downloads, not ureq

[[example]]
name = "download_datasets"
required-features = ["eval-advanced"]  # Uses ureq for dataset downloads

[[example]]
name = "eval"
required-features = ["eval"]

[[example]]
name = "eval_basic"
required-features = ["eval"]

[[example]]
name = "coref"
required-features = ["eval"]

[[example]]
name = "bias"
required-features = ["eval-bias"]

[[example]]
name = "benchmark"
required-features = ["eval-full"]

[[example]]
name = "advanced"
required-features = ["eval"]

[[example]]
name = "models"
required-features = ["onnx"]

[[example]]
name = "hybrid"
required-features = ["onnx"]

[[example]]
name = "abstract_anaphora_eval"
required-features = ["discourse"]

[[example]]
name = "gliner2_multitask"
required-features = ["onnx"]

[[example]]
name = "discourse_pipeline"
required-features = ["discourse"]

[[example]]
name = "production"
required-features = ["production"]

[[example]]
name = "grounded"

[[example]]
name = "temporal_e2e"
required-features = ["eval"]
