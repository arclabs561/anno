# Defaults and Forgotten Tasks Review

**Date**: 2025-01-25  
**Scope**: Review of default configuration values and forgotten tasks from archived documentation

## Default Configuration Review

### TaskEvalConfig Defaults

**Location**: `src/eval/task_evaluator.rs:129-145`

| Setting | Default | Rationale | Assessment |
|---------|---------|-----------|------------|
| `tasks` | `Task::all().to_vec()` | Evaluate all tasks | ‚úÖ **Good** - Comprehensive by default |
| `datasets` | `vec![]` | Empty = all suitable datasets | ‚úÖ **Good** - Flexible, uses all available |
| `backends` | `vec![]` | Empty = all compatible backends | ‚úÖ **Good** - Tests all backends |
| `max_examples` | `None` | No limit | ‚ö†Ô∏è **Questionable** - Could be slow for large datasets |
| `seed` | `Some(42)` | Fixed seed for reproducibility | ‚úÖ **Good** - Reproducible by default |
| `require_cached` | `false` | Allow downloads | ‚úÖ **Good** - User-friendly |
| `relation_threshold` | `0.5` | 50% confidence | ‚úÖ **Good** - Standard threshold |
| `robustness` | `false` | No robustness testing | ‚úÖ **Good** - Expensive, opt-in |
| `compute_familiarity` | `true` | Compute familiarity scores | ‚úÖ **Good** - Useful for zero-shot |
| `temporal_stratification` | `false` | No temporal breakdown | ‚úÖ **Good** - Requires metadata |
| `confidence_intervals` | `true` | Compute CIs | ‚úÖ **Good** - Better reporting |

**Recommendations**:
1. ‚ö†Ô∏è **Consider adding `max_examples: Some(1000)` by default** - Prevents accidentally running on huge datasets
2. ‚úÖ All other defaults are reasonable

### EvalConfig Defaults (Harness)

**Location**: `src/eval/harness.rs:80-93`

| Setting | Default | Rationale | Assessment |
|---------|---------|-----------|------------|
| `max_examples_per_dataset` | `0` (unlimited) | No limit | ‚ö†Ô∏è **Same concern as above** |
| `breakdown_by_difficulty` | `true` | Include difficulty breakdown | ‚úÖ **Good** - Useful analysis |
| `breakdown_by_domain` | `true` | Include domain breakdown | ‚úÖ **Good** - Useful analysis |
| `breakdown_by_type` | `true` | Include type breakdown | ‚úÖ **Good** - Useful analysis |
| `warmup` | `true` | Run warmup iteration | ‚úÖ **Good** - Accurate timing |
| `warmup_iterations` | `1` | Single warmup | ‚úÖ **Good** - Reasonable |
| `min_confidence` | `None` | No filtering | ‚úÖ **Good** - See all predictions |
| `cache_dir` | `None` | Use default cache | ‚úÖ **Good** - Standard behavior |
| `normalize_types` | `false` | Preserve original types | ‚úÖ **Good** - Preserve dataset semantics |

**Recommendations**:
1. ‚ö†Ô∏è **Consider adding `max_examples_per_dataset: 1000` by default** - Same concern

### Statistical Constants

**Location**: `src/eval/task_evaluator.rs:30-40`

| Constant | Value | Rationale | Assessment |
|----------|-------|-----------|------------|
| `DEFAULT_Z_SCORE_95` | `1.96` | 95% CI z-score | ‚úÖ **Correct** - Standard value |
| `DEFAULT_PLACEHOLDER_STD_DEV` | `0.05` | Placeholder when variance unknown | ‚ö†Ô∏è **Arbitrary** - Should be documented |
| `MAX_CI_SAMPLE_SIZE` | `100` | Max samples for CI computation | ‚úÖ **Good** - Performance vs accuracy tradeoff |
| `MIN_CI_SAMPLE_SIZE` | `1` | Min samples for CI | ‚ö†Ô∏è **Questionable** - CI with n=1 is meaningless |
| `ROBUSTNESS_TEST_LIMIT` | `50` | Max examples for robustness | ‚úÖ **Good** - Performance limit |

**Recommendations**:
1. ‚ö†Ô∏è **Change `MIN_CI_SAMPLE_SIZE` to `2`** - CI requires at least 2 samples
2. üìù **Document `DEFAULT_PLACEHOLDER_STD_DEV`** - Explain why 0.05 was chosen

## Forgotten Tasks from Archived Docs

### High Priority (From `REMAINING_WORK_SUMMARY.md`)

#### 1. Complete Per-Example Score Integration ‚ö†Ô∏è **PARTIALLY DONE**

**Status**: Infrastructure exists, but integration incomplete

**What's Done**:
- ‚úÖ `per_example_scores` tracked in `evaluate_ner_task`
- ‚úÖ `compute_stratified_metrics_from_scores()` function exists
- ‚úÖ `compute_confidence_intervals_from_scores()` function exists
- ‚úÖ Per-example scores cached in `per_example_scores_cache`

**What's Needed**:
- ‚ö†Ô∏è Currently uses cached scores when available, but could be more efficient
- ‚ö†Ô∏è Need to verify that stratified metrics use per-example scores when available

**Current Status**: **Mostly complete** - The code does use per-example scores when available (see `task_evaluator.rs:595-609`), but could be optimized.

**Action**: Verify integration is working correctly, add tests if needed.

#### 2. Temporal Metadata Structure ‚ö†Ô∏è **STRUCTURE READY, DATA MISSING**

**Status**: Framework ready, needs data source

**What's Done**:
- ‚úÖ `StratifiedMetrics.by_temporal_stratum` field exists
- ‚úÖ `compute_temporal_stratification()` function exists
- ‚úÖ Structure ready for temporal stratification

**What's Needed**:
- ‚ùå Add temporal metadata to `LoadedDataset` or dataset loaders
- ‚ùå Entity creation date tracking
- ‚ùå KB version metadata
- ‚ùå Temporal stratum assignment logic

**Action**: Add optional `temporal_metadata: Option<TemporalMetadata>` to `LoadedDataset`.

### Medium Priority

#### 3. Embedding-Based Familiarity Integration ‚ùå **NOT DONE**

**Status**: Function exists, not integrated

**What's Done**:
- ‚úÖ `LabelShift::from_type_sets_with_embeddings()` function exists
- ‚úÖ Embedding computation infrastructure available

**What's Needed**:
- ‚ùå Integration with encoder backends for label embeddings
- ‚ùå Automatic embedding computation for familiarity
- ‚ùå Fallback to string-based if embeddings unavailable

**Action**: Integrate embedding-based familiarity when encoder backends are available.

#### 4. KB Version Tracking ‚ùå **NOT DONE**

**Status**: Not started

**What's Needed**:
- ‚ùå KB version metadata in datasets
- ‚ùå URI validation against current KB
- ‚ùå Emerging entity (NIL) separation
- ‚ùå URI set expansion (owl:sameAs links)

**Action**: Add `kb_version: Option<String>` to dataset metadata.

### Low Priority

#### 5. Inter-Doc Coref Specific Evaluation ‚ùå **NOT DONE**

**Status**: Not implemented

**What's Needed**:
- ‚ùå Distinction between intra-doc and cross-doc coref
- ‚ùå Cross-doc specific metrics
- ‚ùå Generalization validation (train/test domain split)

**Action**: Add `coref_type: IntraDoc | CrossDoc` to coref datasets.

#### 6. Improve Confidence Interval Efficiency ‚ö†Ô∏è **MOSTLY DONE**

**Status**: Works but could be optimized

**What's Done**:
- ‚úÖ CI computation from per-example scores (when available)
- ‚úÖ Fallback to sampling and recomputation

**What's Needed**:
- ‚ö†Ô∏è Currently uses cached per-example scores, which is good
- ‚ö†Ô∏è Could avoid recomputation entirely if scores are always available

**Action**: Verify that CI computation always uses cached scores when available.

### Box Embeddings Evaluation Gaps (From `BOX_EVALUATION_GAPS.md`)

#### 1. Standard Coreference Metrics ‚ùå **NOT DONE**

**Status**: Missing standard metrics

**What's Missing**:
- ‚ùå MUC (link-based)
- ‚ùå B¬≥ (mention-based)
- ‚ùå CEAF-e/m (entity/mention alignment)
- ‚ùå LEA (link-based entity-aware)
- ‚ùå BLANC (rand-index based)
- ‚ùå CoNLL F1 (standard benchmark)
- ‚ùå Chain-length stratification

**Action**: Add `BoxCorefResolver` to `TaskEvaluator` and use standard metrics.

#### 2. Integration with Evaluation Framework ‚ùå **NOT DONE**

**Status**: `BoxCorefResolver` not integrated

**What's Needed**:
- ‚ùå `TaskEvaluator` support for `BoxCorefResolver`
- ‚ùå Comparison with other resolvers
- ‚ùå Standard benchmark evaluation

**Action**: Add `BoxCorefResolver` to evaluation framework.

#### 3. Standard Benchmark Evaluation ‚ùå **NOT DONE**

**Status**: Datasets available but not evaluated

**What's Needed**:
- ‚ùå GAP test set evaluation
- ‚ùå PreCo dataset evaluation
- ‚ùå CoNLL-2012 evaluation (if available)
- ‚ùå LitBank evaluation

**Action**: Evaluate box embeddings on standard benchmarks.

## Summary of Recommendations

### Immediate Actions (High Priority)

1. **Change `MIN_CI_SAMPLE_SIZE` to `2`** - CI with n=1 is statistically meaningless
2. **Consider default `max_examples: Some(1000)`** - Prevent accidentally slow runs
3. **Verify per-example score integration** - Ensure it's working optimally
4. **Add temporal metadata structure** - Framework ready, just needs data

### Medium Priority

1. **Integrate embedding-based familiarity** - When encoder backends available
2. **Add KB version tracking** - For NED evaluation
3. **Add BoxCorefResolver to evaluation** - Standard metrics integration

### Low Priority

1. **Inter-doc coref evaluation** - Specialized use case
2. **Document placeholder std_dev** - Explain rationale
3. **Standard benchmark evaluation for boxes** - Research priority

## Default Value Changes Proposed

```rust
// In TaskEvalConfig::default()
max_examples: Some(1000),  // Instead of None - prevent slow runs

// In constants
const MIN_CI_SAMPLE_SIZE: usize = 2;  // Instead of 1 - statistical validity
```

## Status Summary

- ‚úÖ **Defaults are mostly reasonable** - Only minor improvements needed
- ‚ö†Ô∏è **Per-example score integration** - Mostly done, verify completeness
- ‚ùå **Temporal metadata** - Structure ready, needs data
- ‚ùå **Box embeddings evaluation** - Significant gaps remain
- ‚ùå **Embedding-based familiarity** - Not integrated
- ‚ùå **KB version tracking** - Not started

Most forgotten tasks are **enhancements** rather than **critical bugs**. The core evaluation system is complete and working.

