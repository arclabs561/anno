# Performance Analysis & Optimization Opportunities

**Date**: 2025-01-25  
**Scope**: Evaluation framework performance bottlenecks and ONNX errors

## Current Performance

### Sequential Processing
- **Current**: Sentences processed one-by-one in a `for` loop
- **Location**: `src/eval/task_evaluator.rs:363-392`
- **Impact**: No parallelization, CPU underutilized

### Timing Analysis
- **bert_onnx**: ~1.6 seconds per example (50 examples in ~80 seconds)
- **gliner_onnx**: Similar timing
- **stacked**: Much faster (no ML inference)

## Performance Bottlenecks

### 1. Sequential Sentence Processing

**Status:** âœ… **FIXED** - Parallel processing implemented via `eval-parallel` feature

**Previous Code:**
```rust
// Process each sentence
for sentence in &dataset_data.sentences {
    let text = sentence.text();
    // ... extract gold entities ...
    
    // Run inference (blocking, sequential)
    let entities = backend.extract_entities(&text, None)?;
    all_predicted.extend(entities);
}
```

**Problems (Fixed):**
- âœ… No parallelization - sentences processed one at a time â†’ **Fixed with rayon**
- âœ… ONNX inference is CPU-bound but single-threaded â†’ **Fixed with parallel processing**
- âœ… Backend sessions are locked per inference (Mutex contention) â†’ **Fixed with thread-local caching**

**Current Implementation:**
1. **Parallel sentence processing** using `rayon` (when `eval-parallel` feature enabled):
   - Uses `par_iter()` to process sentences across CPU cores
   - Thread-local storage caches zero-shot backends per thread
   - Expected 2-4x speedup on multi-core systems
   - Maintains backward compatibility (sequential fallback)

2. **Batch processing** (Future optimization):
   - Some backends (GLiNER) support batching
   - Process multiple sentences in one ONNX call
   - Reduces session lock contention

3. **Session pooling** (Available via `session-pool` feature):
   - Multiple ONNX sessions for concurrent inference
   - Reduces Mutex contention

### 2. ONNX Session Locking

**Current Code:**
```rust
let mut session_guard = session
    .lock()
    .map_err(|e| Error::Retrieval(format!("Failed to lock session: {}", e)))?;
```

**Problems:**
- Single Mutex per backend instance
- Sequential inference even if CPU has multiple cores
- Lock held during entire inference (tokenization + ONNX + decoding)

**Optimization:**
- Use `parking_lot::Mutex` (already in dependencies) for better performance
- Or use session pooling to have multiple sessions

### 3. Zero-Shot Backend Recreation

**Status:** âœ… **FIXED** - Backend caching implemented with thread-local storage

**Previous Code:**
```rust
// Creates backend once per evaluation (good)
let zero_shot_backend: Option<Box<dyn std::any::Any>> = 
    if is_zero_shot && !mapped_labels.is_empty() {
        Some(Self::create_zero_shot_backend(backend_name)?)
    } else {
        None
    };
```

**Current Implementation:**
- âœ… Sequential path: Backend cached once per evaluation run
- âœ… Parallel path: Thread-local storage caches backend per thread
- âœ… Avoids recreating ONNX sessions for each sentence
- âœ… Fixes ONNX "Missing Input" errors from session recreation

### 4. Gold Entity Extraction

**Current Code:**
```rust
all_gold.extend(gold_entities.iter().map(|g| {
    let mut entity = Entity::new(g.text.clone(), g.entity_type.clone(), g.start, g.end, 1.0);
    entity.provenance = Some(crate::Provenance::ml("gold", 1.0));
    entity
}));
```

**Problems:**
- Cloning entity data for each sentence
- Could be done in parallel with inference

**Optimization:**
- Extract gold entities in parallel with inference
- Or pre-extract all gold entities before inference loop

## ONNX Errors

### NuNER: Missing Input: span_mask

**Status:** âœ… **FIXED** - Dynamic input detection and span tensor generation

**Error (Fixed):**
```
ONNX inference failed: Non-zero status code returned while running Unsqueeze node.
Name:'/Unsqueeze_16' Status Message: Missing Input: span_mask
```

**Root Cause:**
- Some NuNER ONNX models require `span_mask` and `span_idx` inputs
- Previous implementation only provided 4 inputs (token mode)
- Missing: `span_mask` and `span_idx` for span-based models

**Solution Implemented:**
- âœ… Added `make_span_tensors()` static method (similar to GLiNER)
- âœ… Dynamic input detection: checks `session.inputs` to determine required inputs
- âœ… Generates span tensors only when model requires them
- âœ… Falls back to 4-input token mode if model doesn't need spans
- âœ… Added `MAX_SPAN_WIDTH` constant (12, matching GLiNER)

**Location:**
- `src/backends/nuner.rs` - `extract()` method and `make_span_tensors()` function

**Current Code:**
```rust
// Dynamically check model inputs
let session_inputs: Vec<&str> = session_guard
    .inputs
    .iter()
    .map(|i| i.name.as_str())
    .collect();
let needs_span_tensors = session_inputs.contains(&"span_idx") 
    && session_inputs.contains(&"span_mask");

if needs_span_tensors {
    // Generate and provide span tensors
    let (span_idx, span_mask) = Self::make_span_tensors(text_words.len());
    // ... add to ONNX inputs
} else {
    // Token mode - only 4 inputs
}
```

## Recommended Optimizations

### âœ… Completed Optimizations

1. **âœ… Fix NuNER ONNX Inputs** (Critical) - **COMPLETED**
   - Dynamic input detection implemented
   - Span tensor generation added
   - See `src/backends/nuner.rs` for implementation

2. **âœ… Add Parallelization** (High Impact) - **COMPLETED**
   - `rayon` integration via `eval-parallel` feature
   - Thread-local backend caching
   - Expected speedup: 2-4x on multi-core systems
   - See `src/eval/task_evaluator.rs` for implementation

3. **âœ… Add Progress Reporting** - **COMPLETED**
   - Real-time progress updates (sentence count, percentage)
   - Shows backend name and dataset name
   - Updates every 10% or every 10 sentences

### Future Optimizations

### Priority 1: Batch Processing (Medium Impact)
- Implement batch inference for backends that support it
- Process multiple sentences in one ONNX call
- Expected speedup: 1.5-2x
- **Status**: Not yet implemented

### Priority 2: Session Pooling (Medium Impact)
- Use `session-pool` feature for multiple ONNX sessions
- Reduces Mutex contention
- Expected speedup: 1.2-1.5x
- **Status**: Feature available but not integrated into evaluation framework

### Priority 3: Optimize Gold Entity Extraction (Low Impact)
- Pre-extract or parallelize gold entity collection
- Minor improvement, but easy to implement
- **Status**: Gold entities already extracted in parallel path

## Performance Improvements Summary

| Optimization | Status | Speedup | Effort | Priority |
|--------------|--------|---------|--------|----------|
| Fix NuNER ONNX | âœ… Completed | N/A (fixes errors) | Medium | Critical |
| Parallelization | âœ… Completed | 2-4x | Low | High |
| Progress Reporting | âœ… Completed | N/A (UX improvement) | Low | High |
| Batch processing | ðŸ”„ Future | 1.5-2x | Medium | Medium |
| Session pooling | ðŸ”„ Future | 1.2-1.5x | Low | Medium |
| Gold entity opt | âœ… Completed | Included in parallel | Low | Low |

**Current Combined Speedup**: 2-4x on multi-core systems (with `eval-parallel` feature)

**Future Combined Speedup**: 3-6x on multi-core systems (with batch processing + session pooling)

